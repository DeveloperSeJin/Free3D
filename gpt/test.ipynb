{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544efccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b26c1f3a6f46e0bd824866d22fd002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c44bd44df464c43bb588d6a6337260e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/617k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e489f7678c44cfebf8bdcafcaa3fd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/131k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikiann/bn to /home/user/.cache/huggingface/datasets/wikiann/bn/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9fdcdb0b5341b796b0d40ed793d5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/234M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikiann downloaded and prepared to /home/user/.cache/huggingface/datasets/wikiann/bn/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfabd9596624aa0bababcfdcdc2fef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikiann\", \"bn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a7218fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33edc2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd92d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7f6f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "#Get the values for input_ids, token_type_ids, attention_mask\n",
    "def tokenize_adjust_labels(all_samples_per_split):\n",
    "    tokenized_samples = tokenizer.batch_encode_plus(all_samples_per_split[\"tokens\"], is_split_into_words=True)\n",
    "    #tokenized_samples is not a datasets object so this alone won't work with Trainer API, hence map is used \n",
    "    #so the new keys [input_ids, labels (after adjustment)]\n",
    "    #can be added to the datasets dict for each train test validation split\n",
    "    print('tokenized_samples')\n",
    "    print(tokenized_samples)\n",
    "    total_adjusted_labels = []\n",
    "    print(len(tokenized_samples[\"input_ids\"]))\n",
    "    print('-' * 50)\n",
    "    for k in range(0, len(tokenized_samples[\"input_ids\"])):\n",
    "        prev_wid = -1\n",
    "        word_ids_list = tokenized_samples.word_ids(batch_index=k)\n",
    "        existing_label_ids = all_samples_per_split[\"ner_tags\"][k]\n",
    "        i = -1\n",
    "        adjusted_label_ids = []\n",
    "   \n",
    "        for wid in word_ids_list:\n",
    "            if(wid is None):\n",
    "                adjusted_label_ids.append(-100)\n",
    "            elif(wid!=prev_wid):\n",
    "                i = i + 1\n",
    "                adjusted_label_ids.append(existing_label_ids[i])\n",
    "                prev_wid = wid\n",
    "            else:\n",
    "                label_name = label_names[existing_label_ids[i]]\n",
    "                adjusted_label_ids.append(existing_label_ids[i])\n",
    "        \n",
    "        total_adjusted_labels.append(adjusted_label_ids)\n",
    "    tokenized_samples[\"labels\"] = total_adjusted_labels\n",
    "    return tokenized_samples\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_adjust_labels, batched=True)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a3d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef96db65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msejinp98\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/wsl/NER/wandb/run-20230510_144109-vkei3ewg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sejinp98/my-awesome-project/runs/vkei3ewg' target=\"_blank\">dulcet-galaxy-1</a></strong> to <a href='https://wandb.ai/sejinp98/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sejinp98/my-awesome-project' target=\"_blank\">https://wandb.ai/sejinp98/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sejinp98/my-awesome-project/runs/vkei3ewg' target=\"_blank\">https://wandb.ai/sejinp98/my-awesome-project/runs/vkei3ewg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b1e6e9d8244016a6322452fc764ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.013 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.085605‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà</td></tr><tr><td>loss</td><td>‚ñÜ‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.83111</td></tr><tr><td>loss</td><td>0.16237</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dulcet-galaxy-1</strong> at: <a href='https://wandb.ai/sejinp98/my-awesome-project/runs/vkei3ewg' target=\"_blank\">https://wandb.ai/sejinp98/my-awesome-project/runs/vkei3ewg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230510_144109-vkei3ewg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958270c",
   "metadata": {},
   "source": [
    "# NER Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c15935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 20:37:05.028993: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-20 20:37:05.889342: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-20 20:37:07.340737: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-20 20:37:07.340972: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-20 20:37:07.340988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-20 20:37:15.188187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-20 20:37:15.193044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-20 20:37:15.193624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-20 20:37:15.198046: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-20 20:37:15.204431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-20 20:37:15.204987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-20 20:37:15.205498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-20 20:37:21.993559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-20 20:37:22.000636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-20 20:37:22.000712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-05-20 20:37:22.001801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-20 20:37:22.002632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3472 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "All TF 2.0 model weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"basakdemirok/my_furniture_ner_model\")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"basakdemirok/my_furniture_ner_model\", from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a3f6503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "recognizer = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e938fa23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-FUR',\n",
       "  'score': 0.67430717,\n",
       "  'index': 1,\n",
       "  'word': 'white',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity': 'I-FUR',\n",
       "  'score': 0.49156886,\n",
       "  'index': 2,\n",
       "  'word': '##chai',\n",
       "  'start': 5,\n",
       "  'end': 9}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognizer('whitechair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45527cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', ' black', ' chair']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.split(r'\\t', 'a\\t black\\t chair')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5070f3",
   "metadata": {},
   "source": [
    "# Test Fine Tuning NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec89cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def read_wnut(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text().strip()\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "texts, tags = read_wnut('wnut17train.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40cc24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'two', 'weeks', '.', 'Empire', 'State', 'Building']\n",
      "['O', 'O', 'O', 'O', 'B-location', 'I-location', 'I-location']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0][10:17], tags[0][10:17], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9b3362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2571998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc2191c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b41f70ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c48375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4309cde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@aplusk',\n",
       " 'What',\n",
       " 'made',\n",
       " 'you',\n",
       " 'laugh',\n",
       " '?',\n",
       " 'This',\n",
       " 'did',\n",
       " ',',\n",
       " 'courtesy',\n",
       " 'of',\n",
       " '@rainnwilson',\n",
       " 'http://www.officetally.com/rainn-wilson-and-friends-seattle-oct-23']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb7ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set(tag for doc in tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8759ea5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-person': 0,\n",
       " 'B-corporation': 1,\n",
       " 'B-creative-work': 2,\n",
       " 'I-group': 3,\n",
       " 'B-product': 4,\n",
       " 'B-location': 5,\n",
       " 'I-location': 6,\n",
       " 'I-creative-work': 7,\n",
       " 'O': 8,\n",
       " 'I-person': 9,\n",
       " 'I-corporation': 10,\n",
       " 'I-product': 11,\n",
       " 'B-group': 12}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72f70c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-person',\n",
       " 1: 'B-corporation',\n",
       " 2: 'B-creative-work',\n",
       " 3: 'I-group',\n",
       " 4: 'B-product',\n",
       " 5: 'B-location',\n",
       " 6: 'I-location',\n",
       " 7: 'I-creative-work',\n",
       " 8: 'O',\n",
       " 9: 'I-person',\n",
       " 10: 'I-corporation',\n",
       " 11: 'I-product',\n",
       " 12: 'B-group'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de8172e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with OpenSSL 1.1.0h  27 Mar 2018. See: https://github.com/urllib3/urllib3/issues/2168",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13840\\3031819718.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistilBertTokenizerFast\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDistilBertTokenizerFast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'distilbert-base-cased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_split_into_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mval_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_split_into_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\transformers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m from .utils import (\n\u001b[0;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\transformers\\dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\transformers\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m )\n\u001b[1;32m---> 30\u001b[1;33m from .generic import (\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\transformers\\utils\\generic.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mimport_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_flax_available\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_torch_fx_proxy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimportlib_metadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\transformers\\utils\\logging.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhf_hub_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mauto\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtqdm_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\huggingface_hub\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_chunk_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchunk_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_datetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparse_datetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m from ._errors import (\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mBadRequestError\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mEntryNotFoundError\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrequests\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mResponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_fixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\requests\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0murllib3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRequestsDependencyWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\urllib3\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m     ):  # Defensive:\n\u001b[0;32m     38\u001b[0m         raise ImportError(\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[1;34m\"urllib3 v2.0 only supports OpenSSL 1.1.1+, currently \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[1;34mf\"the 'ssl' module is compiled with {ssl.OPENSSL_VERSION}. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;34m\"See: https://github.com/urllib3/urllib3/issues/2168\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with OpenSSL 1.1.0h  27 Mar 2018. See: https://github.com/urllib3/urllib3/issues/2168"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val_texts)):\n",
    "    print(len(val_texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c19d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17485678",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db937e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_encodings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d039c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_encodings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09009550",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f97d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb063e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class WNUTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = WNUTDataset(train_encodings, train_labels)\n",
    "val_dataset = WNUTDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd10c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154452af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing import *\n",
    "\n",
    "train_dataset, val_dataset, unique_tags = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a55dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b39c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './results/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c85a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))\n",
    "model.load_state_dict(torch.load('./results/model.h5'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86dcb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "recognizer = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "recognizer('the building')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dcf0e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForTokenClassification, AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        try:\n",
    "            optim.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        except:\n",
    "            print(len(input_ids))\n",
    "            print(len(attention_mask))\n",
    "            print(len(labels))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings.__getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb672d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_encodings.__getitem__('input_ids'))\n",
    "# input_ids\n",
    "# attention_mask\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15529f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_encodings.__getitem__('attention_mask'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6344d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_encodings.__getitem__('labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35856d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09897b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=15, shuffle=True)\n",
    "\n",
    "num = 0\n",
    "for batch in train_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    print(len(input_ids))\n",
    "    print(len(attention_mask))\n",
    "    print(len(labels))\n",
    "    print('-' * 70)\n",
    "    num += 1\n",
    "\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98f97c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 90])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d85a4581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encodings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5da9ed83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8cf14dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_encodings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58b6b0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26c8e621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=78, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "447f9f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=78, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_encodings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd493b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4face90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9877f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_encodings)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ae904e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(val_encodings)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc38de90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_encodings)):\n",
    "    if len(train_encodings[i]) != 90:\n",
    "        print(i)\n",
    "        print(train_encodings[i])\n",
    "    else :\n",
    "        print(len(train_encodings[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf586d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "79\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(val_encodings)):\n",
    "    if len(val_encodings[i]) != 79:\n",
    "        print(i)\n",
    "        print(val_encodings[i])\n",
    "    else :\n",
    "        print(len(val_encodings[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f84307ed",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_labels)):\n",
    "    if len(train_labels[i]) != 90:\n",
    "        print(i)\n",
    "        print(train_labels[i])\n",
    "    else :\n",
    "        print(len(train_labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3310bc1",
   "metadata": {},
   "source": [
    "# Test NER Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eceefd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TreatSentence import NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5056738",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = \"DeveloperSejin/NER_for_furniture_3D_object_create\"\n",
    "ner = NER(ner_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b991ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity': 'O',\n",
       " 'score': 0.9998679,\n",
       " 'index': 1,\n",
       " 'word': 'A',\n",
       " 'start': 1,\n",
       " 'end': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.return_dic(' A white velvet upholstered armchair with curved armrests: This luxurious armchair is upholstered in a silky white velvet fabric and has curved armrests that provide additional comfort and support. The chair has a sturdy wooden frame and is perfect for a cozy reading corner or a luxurious bedroom.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5e79872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-color',\n",
       " 'B-design',\n",
       " 'B-furniture',\n",
       " 'B-material',\n",
       " 'I-furniture',\n",
       " 'I-material',\n",
       " 'O'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.get_tag('A white velvet upholstered armchair with curved armrests: This luxurious armchair is upholstered in a silky white velvet fabric and has curved armrests that provide additional comfort and support. The chair has a sturdy wooden frame and is perfect for a cozy reading corner or a luxurious bedroom.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f77e7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mner\u001b[49m\u001b[38;5;241m.\u001b[39mget_missing_tags(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA white velvet upholstered armchair with curved armrests: This luxurious armchair is upholstered in a silky white velvet fabric and has curved armrests that provide additional comfort and support. The chair has a sturdy wooden frame and is perfect for a cozy reading corner or a luxurious bedroom.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ner' is not defined"
     ]
    }
   ],
   "source": [
    "ner.get_missing_tags('A white velvet upholstered armchair with curved armrests: This luxurious armchair is upholstered in a silky white velvet fabric and has curved armrests that provide additional comfort and support. The chair has a sturdy wooden frame and is perfect for a cozy reading corner or a luxurious bedroom.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e24079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sejin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from Getprompt import gpt\n",
    "\n",
    "ner_model = \"DeveloperSejin/NER_for_furniture_3D_object_create\"\n",
    "gpt = gpt('', ner_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898c15ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. A plush loveseat with tufted detailing: This cozy loveseat comes with cushions upholstered in a soft, velvety material that will make you never want to leave. Its tufted detailing adds a touch of elegance and sophistication to any space.\n",
      "\n",
      "2. An oversized ottoman with hidden storage: This ottoman not only serves as a comfortable footrest but also has a hidden storage compartment to keep your living room clutter-free. Its oversized design makes it perfect for extra seating or as a makeshift coffee table.\n",
      "\n",
      "3. A mid-century modern-inspired sofa with sleek lines: This sofa has a minimalist design that channels mid-century modern style. Its sharp, clean lines and tapered legs lend an air of sophistication to any living room.\n",
      "\n",
      "4. A classic wingback chair in bold paisley print: This classic wingback chair gets a modern update with a bold paisley print. Its high back and slightly angled arms provide a comfortable and stylish seating option for any room.\n",
      "\n",
      "5. A rustic farmhouse bench with distressed finish: This sturdy bench has a distressed finish that exudes rustic charm. Perfect for the entryway or as additional seating in the dining room, this bench adds a touch of warmth and character to any space.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "['1. A plush loveseat with tufted detailing', 'This cozy loveseat comes with cushions upholstered in a soft, velvety material that will make you never want to leave. Its tufted detailing adds a touch of elegance and sophistication to any space.']\n",
      "--------------------------------------------------\n",
      "['2. An oversized ottoman with hidden storage', 'This ottoman not only serves as a comfortable footrest but also has a hidden storage compartment to keep your living room clutter-free. Its oversized design makes it perfect for extra seating or as a makeshift coffee table.']\n",
      "--------------------------------------------------\n",
      "['3. A mid-century modern-inspired sofa with sleek lines', 'This sofa has a minimalist design that channels mid-century modern style. Its sharp, clean lines and tapered legs lend an air of sophistication to any living room.']\n",
      "--------------------------------------------------\n",
      "['4. A classic wingback chair in bold paisley print', 'This classic wingback chair gets a modern update with a bold paisley print. Its high back and slightly angled arms provide a comfortable and stylish seating option for any room.']\n",
      "--------------------------------------------------\n",
      "['5. A rustic farmhouse bench with distressed finish', 'This sturdy bench has a distressed finish that exudes rustic charm. Perfect for the entryway or as additional seating in the dining room, this bench adds a touch of warmth and character to any space.']\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"recommend\": \"-size\\\\n\\\\n-design\\\\n\\\\n\", \"detail\": {\"detail0\": {\"prompt\": \"1. A plush loveseat with tufted detailing\", \"detail\": \"This cozy loveseat comes with cushions upholstered in a soft, velvety material that will make you never want to leave. Its tufted detailing adds a touch of elegance and sophistication to any space.\"}, \"detail1\": {\"prompt\": \"2. An oversized ottoman with hidden storage\", \"detail\": \"This ottoman not only serves as a comfortable footrest but also has a hidden storage compartment to keep your living room clutter-free. Its oversized design makes it perfect for extra seating or as a makeshift coffee table.\"}, \"detail2\": {\"prompt\": \"3. A mid-century modern-inspired sofa with sleek lines\", \"detail\": \"This sofa has a minimalist design that channels mid-century modern style. Its sharp, clean lines and tapered legs lend an air of sophistication to any living room.\"}, \"detail3\": {\"prompt\": \"4. A classic wingback chair in bold paisley print\", \"detail\": \"This classic wingback chair gets a modern update with a bold paisley print. Its high back and slightly angled arms provide a comfortable and stylish seating option for any room.\"}, \"detail4\": {\"prompt\": \"5. A rustic farmhouse bench with distressed finish\", \"detail\": \"This sturdy bench has a distressed finish that exudes rustic charm. Perfect for the entryway or as additional seating in the dining room, this bench adds a touch of warmth and character to any space.\"}}}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'A white velvet upholstered armchair with curved armrests: This luxurious armchair is upholstered in a silky white velvet fabric and has curved armrests that provide additional comfort and support. The chair has a sturdy wooden frame and is perfect for a cozy reading corner or a luxurious bedroom.'\n",
    "gpt.getAnswer(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792de332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
